http://thesecretlivesofdata.com/raft/
what is Distributed Consensus?

let's say we have a single node system.
for this example, you can think of our node as a database server that stores a single value.
we also have a client that can send a value to the server.
coming to agreement, or consensus, on that value is easy with one node.
but how do we come to consensus if we have multiple nodes?
that's the problem of distributed consensus.


Raft is a protocol for implementing distributed consensus.
let's look at a high level overview of how it works.

A node can be in 1 of 3 states:
The Follewer state;
The Candidate state;
The Leader state.

All our nodes start in the follower state.
If followers don't hear from a leader then they can become a candidate.
the candidate then requests votes from other nodes.
Nodes will reply with their vote.
The candidate becomes the leader if it gets votes from a majority of nodes.
this process is called Leader Election.
All changes to the system now go through the leader.
Each change is added as an entry in the node's log.
this log entry is currently uncommitted so it won't update the node's value.
then the leader waits until a majority of nodes have written the entry.
the entry is now committed on the leader node and the node state is "5".
the leader then notifies the followers that the entry is committed.
the cluster has now come to consensus about the system state.
this process is called Log Replication.


Leader Election

In Raft there are two timeout settings which control elections.
First is the election timeout.
The election timeout is the amount of time a follower waits until becoming a candidate.
the election timeout is randomized to be between 150ms and 300ms.
After the election timeout the follower becomes a candidate and starts a new Election term votes for itself, and sends out Request Vote messages to other nodes.
if the receiving node hasn't voted yet in this term then it votes for the candidate and the node resets its election timeout.
once a candidate has a majority of votes it becomes leader.
the leader begins sending out Append Entries messages to its followers.
These messages are sent in intervals specified by the Heartbeat Timeout.
Followers then respond to each Append Entries message.
This election term will continue until a follower stops receiving heartbeats and becomes a candidate.


Let't stop the leader and watch a re-election happen.

Node B is now leader of term 2
Requiring a majority of votes guarantees that only one leader can be elected per term.

if two nodes become candidates at the same time then a split vote can occur.
let's take a look at a split vote example.
Two nodes both start an election for the same term and each reaches a single follower node before the other.
now each candidate has 2 votes and can receive no more for this term.
node A received a majority of votes in term 5 so it becomes leader.


Log Replication
Once we have a leader elected we need to replicate all changes to our system to all nodes.
this is done by using the same Append Entries message that was used for heartbeats.
let's walk through the process.
first a client sends a change to the leader.
the change is appended to the leader's log.
then the change is sent to the followers on the next heartbeat.
an entry is committed once a majority of followers acknowledge it.
and a response is sent to the client.
Now let's send a command to increment the value by "2".
our system value is now updated to "7".
Raft can even stay consistent in the face of network partitions.
let's add a partition to separate A & B from C, D & E.
because of our partition we now have two leaders in different terms.
let's add another client and try to update both leaders.
one client will try to set the value of node B to "3".
Node B cannot replicate to a majority so its log entry stays uncommitted.
the other client will try to set the value of node C to "8".
this will succeed because it can replicate to a majority.
now let's heal the network partition.
Node B will see the higher election term and step down.
Both nodes A & B will roll back their uncommitted entries and match the new leader's leg.
our log is now consistent across our cluster.

the end

https://raft.github.io/raft.pdf
https://raft.github.io/